######################################################################
# SCALING OF MATH READ AND SCIE
#######################################################################

#Author: Bonaventura Pacileo (bonaventurafrancesco.pacileo@oecd.org)
#Last modified: 13/03/19


#update scale production code to generate unique dataset for results


############################################
######## Initial setup and data loading ####
############################################

#check lib paths
#"//FS-CH-1.main.oecd.org/Users3/pacileo_b/R/win-library/3.4"
.libPaths()
#.libPaths(c("G:/R/win-library/3.4","Z:/R/R-3.4.4"))


#clear the space
rm(list=ls())

# detach(package:dplyr,unload=T)
# detach(package:plyr,unload=T)
# detach(package:broom,unload=T)
# detach(package:magrittr,unload=T)

#packages
library("rmarkdown")
library("fastDummies")
library("xlsx")
library("plyr")
library("dplyr")
library("magrittr")
library("TAM")
library("openxlsx") #for big excel files
library("broom")
library("zoo")
library("intsvy")
library("data.table")
#setting wd
dirPisaForSchools2017<-'V:\\Pacileo_B\\NOBACKUP\\PISA PM\\PFS'

setwd(dirPisaForSchools2017)
getwd()


#FunctionName
#variable.name or variableName

#school size function

SchoolSize<-function(DataSet,SchoolID) {
  
  DataSet<-get(DataSet)

  SchoolCount<- plyr::count(DataSet,SchoolID) 
  min(SchoolCount[2]) %>% print()
  
}


#mode function
FunMode <- function(x) {
  uniqv <- unique(x)
  uniqv[which.max(tabulate(match(x, uniqv)))]
}

#rescale functions
PisaPread<-function(x) {
  pv_factor<-(0.883*(x)-0.4837)/1.1002
  pv_factor*100+500
}

PisaPmath<-function(x) {
  pv_factor<-(x+0.1344)/1.2838
  pv_factor*100+500
}

PisaPscie<-function(x) {
  pv_factor<-(x-0.1797)/1.0724
  pv_factor*100+500
}


PFSscale <-function(domn,resp=resp,Y=Y,xsi.fixed=xsi.fixed,aux=aux) {
  #domn must be a string e.g. domn="read"
  
  if (is.null(domn)){print("domain missing")}
  else {
    
    # marginal model for 1PL
    marginal_model <- tam(resp, xsi.fixed = xsi.fixed, Y = direct_regs, pid=NULL)
    
    # computing plausible values
    PVs <- tam.pv(marginal_model, nplausible = 5)
    
    # use the domain-specific function to transform the logit scale in PISA points
    scale<-parse(text=paste0("PisaP",domn)) %>% eval()
    results <- cbind(scale(PVs$pv[2:6]),aux)
    
    #rename plausible values
    pvnames<-paste('PV',1:5,toupper(domn),sep='') 
    setnames(results, old = c('PV1.Dim1','PV2.Dim1', 'PV3.Dim1', 'PV4.Dim1', 'PV5.Dim1'), new = pvnames)
    
    #creating new object
    return(results)
    #removing exhaust
    rm(pvnames,scale,PVs,marginal_model)
    
  }
  
} 

###########################
#LOAD DATA USING PROMPTS
########################

#andorra files
   # path2cog<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Andorra/Data/2E/SCORED_Stdcogitem.xlsx"
   # path2raw<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Andorra/Data/2E/RAW_Stdcogitem.xlsx"
   # path2pca<-"V:/Pacileo_B/NOBACKUP/PISA PM/PFS/AND_pca.csv"
   # path2gold<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Andorra/Data/2E/StdQ_golddataset17_cleaned.xlsx"

#colombia files
 # path2cog<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/3. Administration in April 2018/1. Original datasets/SCORED_Stdcogitem (6).xlsx" 
 # path2raw<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/3. Administration in April 2018/1. Original datasets/RAW_Stdcogitem (5).xlsx" 
 # path2pca<-"V:/Pacileo_B/NOBACKUP/PISA PM/PFS/COL_pca0419.csv" 
 # path2gold<-'V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/3. Administration in April 2018/1. Original datasets/StdQ_golddataset17(11).xlsx'

#colombia files (NEW)
# path2cog<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/4. Administration in November 2018/Data from ICFES/SCORED_Stdcogitem (7).xlsx"
# path2raw<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/4. Administration in November 2018/Data from ICFES/RAW_Stdcogitem (6).xlsx"
# path2pca<-"V:/Pacileo_B/NOBACKUP/PISA PM/PFS/COL_pca1118.csv"
# path2gold<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/4. Administration in November 2018/Data from ICFES/StdQ_golddataset17 (13).xlsx"


# #colombia files (NEW2)
  path2cog<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/4. Administration in November 2018/Data from ICFES second round/SCORED_Stdcogitem (8).xlsx"
  path2raw<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/4. Administration in November 2018/Data from ICFES second round/RAW_Stdcogitem (7).xlsx"
  path2pca<-"V:/Pacileo_B/NOBACKUP/PISA PM/PFS/COL_pca1118.csv"
  path2gold<-"V:/PISA/BACKUP/PISA/PISA for Schools/4. Countries/Colombia/Data/4. Administration in November 2018/Data from ICFES second round/StdQ_golddataset17 (14).xlsx"


  path2tam<-"//main.oecd.org/ASgenEDU/PISA/BACKUP/PISA/PISA for Schools/11. Item Parameters/Item parameters/TAM_parameters.xlsx"
  TAMread<-read.xlsx(path2tam, sheet="read",colNames=T)
  TAMmath<-read.xlsx(path2tam, sheet="math",colNames=T)
  TAMscie<-read.xlsx(path2tam, sheet="scie",colNames=T)
# #prompts
# path2cog<-readline(prompt="Enter filepath with no quotes: ")
# path2raw<-readline(prompt="Enter filepath with no quotes: ")
# path2pca<-readline(prompt="Enter filepath with no quotes: ")
# path2gold<-readline(prompt="Enter filepath with no quotes: ")
# # 

#scored data
cog_data<-read.xlsx(path2cog, sheet="Code",colNames=T)

#raw data for computing nonreached
raw_data<-read.xlsx(path2raw, sheet="Code",colNames=T,cols=1:143) ##exclude empty cols

#pca dummies
pca_data<-read.csv(path2pca,header=T)

#gold dataset
gold_data<-read.xlsx(path2gold, sheet="Code",colNames=T)

# #TAM parameters (anchor values)
TAMread<-read.xlsx('TAM_parameters.xlsx', sheet="Read",colNames=T)
TAMmath<-read.xlsx('TAM_parameters.xlsx', sheet="Math",colNames=T)
TAMscie<-read.xlsx('TAM_parameters.xlsx', sheet="Scie",colNames=T)

#############################################
########### DATA PREPARATION ################
#############################################


#two cleaning steps


# dirty command to ensure no missings (it does not alter the subsequent analysis)
raw_data %<>% mutate_at(., vars(matches("^P[RSM]\\d{4}Q\\d{2}.?$")), funs(replace(., is.na(.), 9))) 

# dropping duplicated columnS (if any)
gold_data %<>% subset(., select=which(!duplicated(names(.)))) 

#check year
plyr::count(gold_data,"ST003Q03_15")

#check missings among direct regressors
direct.regs %>% is.na(.) %>% rowSums(.) %>% summary(.)

# expected output
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0       0       0       0       0       0 


#ANDORRA SPECIFIC COMMANDS (should go in the data preparation)
   # cog_data<-cog_data[!(cog_data$fullid %in% c(90000001004,90000001006,90000003014,90000003021)),]
   # raw_data<-raw_data[!(raw_data$fullid %in% c(90000001004,90000001006,90000003014,90000003021)),]
   # gold_data$HISEI<-na.aggregate(gold_data$HISEI, list(stidsch = gold_data$stidsch), mean,na.rm=TRUE)
   # cog_data[,'PR6013Q08']<-NULL
   # gold_data$entid<-gold_data$stidsch 
   # raw_data$entid<-gold_data$entid
  
#COLOMBIA SPECIFIC COMMANDS (should go in data preparation)
# 2nd round only
 #cog_data %<>% .[!(cog_data$fullid %in% c('101_011006938',	'101_011107029',	'201_013808675',	'301_013808725',	'401_011107004',	'401_011807522',	'501_010006431',	'601_011407188',	'601_012307818',	'701_011006937')),]
 #raw_data %<>% .[!(raw_data$fullid %in% c('101_011006938',	'101_011107029',	'201_013808675',	'301_013808725',	'401_011107004',	'401_011807522',	'501_010006431',	'601_011407188',	'601_012307818',	'701_011006937')),]
 #gold_data$stidsch<-gold_data$entid


# #COLOMBIA SPECIFIC COMMANDS 3rd round
 gold_data$stidsch<-gold_data$entid
# 
 gold_data$ST004Q01_15 %<>% as.integer(.)
#  
cog_data[,'PR6017Q05T']<-NULL

################
#SCORED DATASET#
################
#removing T's at the end of item names (if any)

colnames(cog_data) <- gsub('T', '', colnames(cog_data), fixed=TRUE)


 ##############
 #GOLD DATASET#
 ##############

#check school size
 SchoolSize("gold_data","entid")


 #dropping duplicated columnS (if any)
 gold_data %<>% subset(., select=which(!duplicated(names(.)))) 
 
#check year
plyr::count(gold_data,"ST003Q03_15")
 

 gold_data %<>%
  setNames(
    plyr::mapvalues(x = names(.),
                    from = c("ST001Q01_15","ST004Q01_15",'W_STU', sprintf('rwgt%d',1:80)), 
                    to = c("GRADE","GENDER",'W_FSTUWT', sprintf('W_FSTR%d',1:80)))
    
  )
 
 ###creating deviation contrast coded dummies for booklets 
 for (i in 1:6) {
   print(i)
   newi<-paste("B",i,sep="")
   
   gold_data$booki=case_when(
     gold_data$bookid != i~ 0,
     gold_data$bookid== i ~ 1
     
   )
   #replacing to -1 if booklet 7
   gold_data[gold_data$bookid==7,] %<>% mutate_at(.,vars(matches('^booki$')),~-1)
   
   names(gold_data)[names(gold_data)=='booki']<-newi
   rm(newi)
 }
 
 
 #creating school dummies with -1 for the largest one
 gold_data %<>% fastDummies::dummy_cols(., select_columns = "stidsch")
 
 #replace to -1
 gold_data[gold_data$stidsch ==FunMode(gold_data[,'stidsch']),] %<>% mutate_at(.,vars(matches('^stidsch_')),~-1)
 
 #drop dummy of the largest one
 gold_data %<>% .[,!(colnames(.)==paste('stidsch_',FunMode(.[,'stidsch']),sep=''))]
 
 #recode gender as 0 1
 gold_data %<>% mutate(.,GENDER=GENDER-1)   #creating 0-1 dummy instead of 1-2
 
 
 ##############
 #RAW DATASET#
 ##############
 
 #computing nonreached items here from raw dataset.
 nitems_data <- cbind(c(1,2,3,4,5,6,7),c(57,61,55,63,63,65,59)) %>% as.data.frame()
 nitems_data %<>% dplyr::rename(., bookid= V1, nitems= V2 )
 
 raw_data %<>% plyr::join(., nitems_data, by='bookid', type = "left", match = "all")
 
 #dirty command to ensure no missings (it does not alter the subsequent analysis)
raw_data %<>% mutate_at(., vars(matches("^P")), funs(replace(., is.na(.), 0))) 

#rowsums of notreached divided by total number of items in the booklet. please ensure every NSP codes this as 'r'
raw_data$nreached <-(rowSums(select(raw_data,matches("^P"))=="r" | select(raw_data,matches("^P"))=="rr" |
                             select(raw_data,matches("^P"))=="rrr" | select(raw_data,matches("^P"))=="rrrr" |
                             select(raw_data,matches("^P"))=="rrrrr"))/raw_data$nitems

summary(raw_data$nreached*100)



# raw_data$missings <-(rowSums(select(raw_data,matches("^P"))==9 | select(raw_data,matches("^P"))==99 |
#                                select(raw_data,matches("^P"))==999 | select(raw_data,matches("^P"))==9999 |
#                                select(raw_data,matches("^P"))==99999))/raw_data$nitems
# 
# summary(raw_data$missings*100)
# aggregate( missings*100 ~ as.integer(entid), data=raw_data, FUN=mean)



########
#domain scored and item parameters datasets
#########

read<-select(cog_data,matches("^PR.*[1-9]"))
math<-select(cog_data,matches("^PM.*[1-9]"))
scie<-select(cog_data,matches("^PS.*[1-9]"))

domains<-c("math","read","scie")

#loop to check item names between anchor values and current dataset ad that items are numeric
for (domn in domains) {
  print(domn)
  
  getdomn<-get(domn)
  
  getdomn %<>% .[,order(names(.))]
  
  #here we check that we have the same item names in the anchor values file and in the cog_data data frame
  #and keep in the anchor values only the matching ones
  TAMdb<-paste("TAM",domn,sep='') %>% get()
  TAM_names<-TAMdb$orden.tam
  # 
  item_list<-grep(paste(TAM_names, collapse = "|"), colnames(getdomn), value = TRUE) 
  TAMdb %<>% filter(.,orden.tam %in% item_list)
  
  # #creating dataframe with item parameters
  anchorValues<-cbind(1:length(TAMdb$orden.tam),TAMdb$tam) %>% as.data.frame()
  anchorValues %<>% dplyr::rename(.,id=V1 , tam=V2)
  # 
  #exporting domain specific data sets with anchor values
  assign(paste0("anchorValues_", domn), anchorValues)
  
  #converting cog data variables to numeric values 
  getdomn %<>% sapply(.,as.numeric) %>% as.data.frame(.)
  
  assign(domn, getdomn)
  
  rm(getdomn,TAMdb,anchorValues,item_list,TAM_names)
} 


########
#direct regressors datasets
#########
#check missings among direct regressors
direct.regs %>% is.na(.) %>% rowSums(.) %>% summary(.)

# expected output
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
# 0       0       0       0       0       0 


###########################
########PCA analysis  #####
###########################

### PCA Dummies + age variables
set.seed(1992) #replicable results
PCA_col<-prcomp(select(pca_data,matches('^AP|^age')),retx = TRUE, center = TRUE,scale. = TRUE) 

#computing cumulative explained variance
PCA_col$Cumulative <- cumsum(PCA_col$sdev^2)/sum(PCA_col$sdev^2) ## square root of the eigen values is sdev
PCA_col$Cumulative

#PC from PCA_colx to the former dataframe of direct regressors
direct_regs %<>% cbind(.,PCA_col$x[,PCA_col$Cumulative<0.95]) 


###############################
####COMPUTING DOMAIN SCALES####
##############################

#update this to generate unique dataset for results
#loop to scale
# for (domn in domains) {
#   
#   print(domn)
#   
#   getdomn<-get(domn)
#   
#   #anchor values dataset
#   anchorValues<-paste("anchorValues_",domn,sep='') %>% get()
#   
#   # marginal model for 1PL
#   marginal_model <- tam(getdomn, xsi.fixed = anchorValues, Y = direct_regs, pid=NULL)
#   
#   # computing plausible values
#   PVs <- tam.pv(marginal_model, nplausible = 5)
#   
#   # use the domain-specific function to transform the logit scale in PISA points
#   scale<-parse(text=paste0("PisaP",domn)) %>% eval()
#   results <- cbind(scale(PVs$pv[2:6]),select(gold_data,matches("^W_FSTUWT$|^W_FSTR\\d+$|^stidsch$|^stidstd$")))  #maybe stidstd useful too?
#   
#   #rename plausible values
#   pvnames<-paste('PV',1:5,toupper(domn),sep='') 
#   setnames(results, old = c('PV1.Dim1','PV2.Dim1', 'PV3.Dim1', 'PV4.Dim1', 'PV5.Dim1'), new = pvnames)
#   
#   #creating domain specific output files ready for intsvy
#   assign(paste0(domn,"Results"), results)
#   
#   rm(pvnames,results,scale,PVs,marginal_model,getdomn)
#   
# } 



# computing mean and top/bottom performers in each domain

SchoolPfmceDS <-gold_data$stidsch %>% unique() %>% as.data.frame() %>% setnames("stidsch") 

for (domn in domains) {
  
  print(domn)
  
  domnResults<-paste(domn,"Results",sep='') %>% get()
  
  
  #mean
  domnMean<-pisa.mean.pv(pvlabel=toupper(domn), by = "stidsch",data =domnResults) #there is also export option

  #top/bot cutoff scores
  domnPERF<-pisa2015.per.pv(pvlabel = toupper(domn),by = 'stidsch', data =domnResults,per = c(25,75)) 

  
#creating dataset
  
  #mean
  domnMean %<>%
    select(.,stidsch,Mean,matches("s.e.")) %>%
    setnames(c("stidsch",paste(domn,"score",sep='_'),paste(domn,"score_se",sep='_'))) %>%
    merge(x=SchoolPfmceDS,y=.,by="stidsch",all=TRUE) %>%
    {.}->SchoolPfmceDS
    
  #bottom
  domnPERF %>%
    filter(Percentiles == 25) %>%
    select(.,stidsch,Score,matches("Std. err.")) %>%
    setNames(c('stidsch',paste("btn_per",domn,sep='_'),paste("btn_per",domn,"se",sep='_'))) %>%
    merge(x = SchoolPfmceDS, y = ., by  ="stidsch", all = TRUE) %>%
    {.}->SchoolPfmceDS
  
  #top
  domnPERF %>%
    filter(Percentiles == 75) %>%
    select(.,stidsch,Score,matches("Std. err.")) %>%
    setNames(c('stidsch',paste("top_per",domn,sep='_'),paste("top_per",domn,"se",sep='_'))) %>%
    merge(x = SchoolPfmceDS, y = ., by  ="stidsch", all = TRUE) %>%
    {.}->SchoolPfmceDS
  

}


SchoolPfmceDS %>%
  write.xlsx(file = 'School_level_GoldDataset_COL.xlsx')

